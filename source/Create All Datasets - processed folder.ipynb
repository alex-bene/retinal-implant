{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.autonotebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the necessary folders to store the datasets if not already there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder Containing All Data\n",
    "DATA_PATH                  = os.path.join(os.getcwd(), 'data')\n",
    "# Raw Data Folders\n",
    "FULL_ARGUSII_RAW_DATA_PATH = os.path.join(DATA_PATH, 'MNIST', 'percept')\n",
    "MNIST_SUBSET_RAW_DATA_PATH = os.path.join(DATA_PATH, 'MNIST_2000_400', 'percept')\n",
    "# Datasets Out Folders\n",
    "FULL_ARGUSII_OUT_DATA_PATH = os.path.join(DATA_PATH, 'MNIST', 'processed')\n",
    "MNIST_SUBSET_OUT_DATA_PATH = os.path.join(DATA_PATH, 'MNIST_2000_400', 'processed')\n",
    "\n",
    "# Create the Folders IF Not Already There\n",
    "if not os.path.exists(FULL_ARGUSII_OUT_DATA_PATH):\n",
    "    os.makedirs(FULL_ARGUSII_OUT_DATA_PATH)\n",
    "if not os.path.exists(MNIST_SUBSET_OUT_DATA_PATH):\n",
    "    os.makedirs(MNIST_SUBSET_OUT_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import pickle\n",
    "\n",
    "def create_dataset(path, out_path, save=True):\n",
    "    # function to \"create\" one sample from its name and path\n",
    "    def one_sample(path, sample_name):\n",
    "        # keep only those with size greater than zero '0'\n",
    "        if os.path.getsize(os.path.join(path, sample_name)) <= 0:\n",
    "            return None\n",
    "\n",
    "        # extract the sample labels from each name\n",
    "        sample_label = int(sample_name.split('-')[1].split('.')[0])\n",
    "        # load sample data\n",
    "        sample_data = torch.load(os.path.join(path, sample_name))\n",
    "\n",
    "        #until I change the way it's saved\n",
    "        sample_data = (sample_data - sample_data.min())/(sample_data.max() - sample_data.min())\n",
    "\n",
    "        # convert data sample to PIL image\n",
    "        sample_data = transforms.ToPILImage()(sample_data)\n",
    "\n",
    "        return sample_data, sample_label\n",
    "\n",
    "    # function to \"create\" a samples list from a folder path\n",
    "    def multiple_samples(path):\n",
    "        print(f\"Create samples list from path: {path}\")\n",
    "        samples_names  = os.listdir(path)\n",
    "        samples_labels = []\n",
    "        samples_data   = []\n",
    "        for sample_name in tqdm(samples_names):\n",
    "            sample = one_sample(path, sample_name)\n",
    "            if sample is None:\n",
    "                continue\n",
    "            sample_data, sample_label = sample\n",
    "            samples_labels.append(sample_label)\n",
    "            samples_data.append(sample_data)\n",
    "\n",
    "        return samples_data, samples_labels\n",
    "\n",
    "    # create path if not already there\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "\n",
    "#     # list the names of each sample in the train/test folders\n",
    "#     train_samples_names = os.listdir(os.path.join(path, 'train'))\n",
    "#     test_samples_names  = os.listdir(os.path.join(path, 'test'))\n",
    "\n",
    "    train_samples_data, train_samples_labels = multiple_samples(os.path.join(path, 'train'))\n",
    "    test_samples_data,  test_samples_labels  = multiple_samples(os.path.join(path, 'test' ))\n",
    "\n",
    "#     for train_sample_name in tqdm(train_samples_names):\n",
    "#         train_sample_data, train_sample_label = one_sample(os.path.join(path, 'train'), train_sample_name)\n",
    "#         train_samples_labels.append(train_sample_label)\n",
    "#         train_samples_data.append(train_sample_data)\n",
    "\n",
    "#     # keep only those with size greater than zero '0'\n",
    "#     train_samples_names = [train_samples_name\n",
    "#                            for train_samples_name in train_samples_names \n",
    "#                            if os.path.getsize(os.path.join(path, 'train', train_samples_name)) > 0]      \n",
    "\n",
    "#     test_samples_names  = [test_samples_name\n",
    "#                            for test_samples_name  in test_samples_names \n",
    "#                            if os.path.getsize(os.path.join(path, 'test' , test_samples_name )) > 0]      \n",
    "#     print(1)\n",
    "\n",
    "#     # extract the samples labels from their name\n",
    "#     train_samples_labels = [int(train_samples_name.split('-')[1].split('.')[0])\n",
    "#                             for train_samples_name in train_samples_names]\n",
    "#     test_samples_labels  = [int(test_samples_name.split('-')[1].split('.')[0])\n",
    "#                             for test_samples_name  in test_samples_names]\n",
    "#     print(1)\n",
    "\n",
    "#     # convert labels to torch.Tensor\n",
    "#     train_samples_labels = torch.Tensor(train_samples_labels)\n",
    "#     test_samples_labels  = torch.Tensor(test_samples_labels)\n",
    "#     print(1)\n",
    "    if save:\n",
    "        # save labels Tensors in their respective files\n",
    "        torch.save(train_samples_labels, os.path.join(out_path, 'train_set_labels.pt'))\n",
    "        torch.save(test_samples_labels,  os.path.join(out_path, 'test_set_labels.pt'))\n",
    "\n",
    "        # save data list in their respective pickles\n",
    "        with open(os.path.join(out_path, 'train_set_data.pk'), \"wb\") as f:\n",
    "            pickle.dump(train_samples_data, f)\n",
    "\n",
    "        with open(os.path.join(out_path, 'test_set_data.pk' ), \"wb\") as f:\n",
    "            pickle.dump(test_samples_data,  f)\n",
    "\n",
    "#     torch.save(train_samples_labels, os.path.join(out_path, 'train_set_labels.pt'))\n",
    "#     torch.save(test_samples_labels,  os.path.join(out_path, 'test_set_labels.pt'))\n",
    "\n",
    "#     print(1)\n",
    "\n",
    "#     # load each sample to a list\n",
    "#     train_samples_data = [torch.load(os.path.join(path, 'train', train_samples_name))\n",
    "#                           for train_samples_name in train_samples_names]\n",
    "#     test_samples_data  = [torch.load(os.path.join(path, 'test',  test_samples_name))\n",
    "#                           for test_samples_name  in test_samples_names]\n",
    "#     print(1)\n",
    "\n",
    "#     # convert each data sample to PIL image\n",
    "#     train_samples_data = [transforms.ToPILImage()(train_sample_data)\n",
    "#                           for train_sample_data in train_samples_data]\n",
    "#     test_samples_data  = [transforms.ToPILImage()(test_sample_data)\n",
    "#                           for test_sample_data  in test_samples_data]\n",
    "#     print(1)\n",
    "\n",
    "#     train_samples_data = torch.stack(train_samples_data)\n",
    "#     test_samples_data  = torch.stack(test_samples_data)\n",
    "\n",
    "#     torch.save(train_samples_data, os.path.join(out_path, 'train_set_data.pt'))\n",
    "#     torch.save(test_samples_data,  os.path.join(out_path, 'test_set_data.pt'))\n",
    "\n",
    "    return ((train_samples_data, train_samples_labels),\n",
    "            (test_samples_data, test_samples_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples_names = os.listdir(os.path.join(MNIST_SUBSET_RAW_DATA_PATH,\n",
    "                                              os.listdir(MNIST_SUBSET_RAW_DATA_PATH)[0],\n",
    "                                              'train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.load(os.path.join(MNIST_SUBSET_RAW_DATA_PATH,\n",
    "                                 os.listdir(MNIST_SUBSET_RAW_DATA_PATH)[0],\n",
    "                                 'train',\n",
    "                                 train_samples_names[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "implant_model_names = os.listdir(MNIST_SUBSET_RAW_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for implant_model_name in implant_model_names:\n",
    "    create_dataset(os.path.join(MNIST_SUBSET_RAW_DATA_PATH, implant_model_name),\n",
    "                   os.path.join(MNIST_SUBSET_OUT_DATA_PATH, implant_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create samples list from path: C:\\Users\\el16077\\OneDrive - central.ntua.gr\\ΕΜΠ\\8ο εξάμηνο\\Τεχνολογίες Κινητής & Ηλεκτρονικής Υγείας\\Εξαμηνιαία\\data\\MNIST\\percept\\AxonMapModel-ArgusII\\train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf0710d31ce46feaac2a9860f25bf9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=60000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Create samples list from path: C:\\Users\\el16077\\OneDrive - central.ntua.gr\\ΕΜΠ\\8ο εξάμηνο\\Τεχνολογίες Κινητής & Ηλεκτρονικής Υγείας\\Εξαμηνιαία\\data\\MNIST\\percept\\AxonMapModel-ArgusII\\test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d60c06793546b8a3197c34c4b0d9fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "(a,b), (c,d) = create_dataset(os.path.join(FULL_ARGUSII_RAW_DATA_PATH, 'AxonMapModel-ArgusII'),\n",
    "                   os.path.join(FULL_ARGUSII_OUT_DATA_PATH, 'AxonMapModel-ArgusII'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACM0lEQVR4nGWS22oTURSG11p778zkNNNMbdLa1J7riSrYohZBe6veeFEo+Cxe+AY+gG8g6IWCIgpeCYq2KBSxFEol9GTbNMkkmUz3zF5etOpE/8v1s1msb38IXUFAREQwMQOAhP+DgiDSBgC7x4gkCFgoCoO4u0QSUlqWErotld+I5fGmlBakbS0VWFnbyeXMdiOv9hEAshLnvo7l1u68K+cr04fegEs/C9+X0qsSQHk0dVct6NpCab69OfO+mNq4/di//tZxESifd+caJa85UoVwrxXsRhQMiXy47LcQVE5Nom2bbCvwG42A0nnXydmFsc6bz0iW6M1wJpbBUbvdISuTdXr6J88PbpZyDyQZS1fTTamPwiiSknSz49cPaouXHz28hFJk4hRTFGnDJBAQGFGqYu/K2S0UiojJmIiZEIAZhLStdMp0fB+FZAJmYwARmIFU9tTI8MTFnZdmSWKMMTAzIzMwkNVTKo+OXpiVz4oeCkBg5hO6ZDmea6EBO1q2W9LwH+ygskGEpYnO6lZgoladE79ChK61J8vXbk3IV83q81DT35IJtDMQb316vXtjcOicEpR4iRZS//juRpQqjGxTtG5MslQZ7UzNHHyrBNTW0K0JigKFZ67cm1x5QR9+QAiQ3Ana8/bXn1Zni9N9SmK3YCI9fLq25/d5FVVtmn/sw7R7s+xXNqs75phK0lvWvDZ/9XB89X7t5PKkmlGw/aT+sTGs8DezZITA0brjftHHTH8BgtroIFbUvlAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x2451B1162B0>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2554]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "255\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACb0lEQVR4nGXSzUtUURgG8Oe85947984dZ3TMUbNG00qMJIw+dkGLINq0sF1/TrvaBP0FQdAioo0UEUSKQhuTQlQ009Rxxo9xHGeuc+eee97TQoWsd/uD54GHVxj8dQw2bCBIEgBY/5nWOibHtf5FAIIMcxQaP+0SxOlYNjo2rILKYaYnTaeQtVYqaoaxFFubHdeyJ7GK7VCilmg2gqjaOIjbE1PF+9YxLdb6xs/piVsz2+6qS3posv3826YFAEqtz8j5b73zqyvzYd/0ncHxh7np0c01YQBWu790UBF16fl+xjKpbLlLlHr3tDBgDkpoKMOOr886bDEJSAJAwoCjfYSlsHtyYPnpi1wtu5POci4hiMkCWLmxWrV35+bfrO8o0rVWu2kECBCGleZgReni9lznzX4v4UjXZpsAQJhYQ23Fev+g1JeXGaFaIumCcIzQtcjUm3s9bb8/PPhZvFfIDhrXBgBigiZfus5Q19ZE8H0mLgTOyaRCgyNSOnS98mzoHLS3eOmMJ+m4EwATA4jUIdUqKwVB3T35pPQJFphw5AKxSuj05ufUxtqTyxh2YQFMzGRYIxR7lEpd9JPh175WFoAwrCXHVgzGYXV/mzs67SSaLRIJwGIwhAJCJ7CrKJV/0LkOQVddYxOEIhUmg2ZL2WuiOpt/XXDH89Hy8wvx7SQsoxAZlI1eyW2cyQ88nrsyYvR6V9KTgFCRrFXa1ur5qZ5or9/NuV4IzQlp24Aw9YgWyB+7Xh0b/Zi8EQ3lbM83R09NSERoW0ip9/2fvgw/i7belZeKjUN9PDzqllq6tLp492V+5NUjKow0rAwcAoA/MjxK3MeB0VYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x2451B0A5160>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]\n",
    "import numpy as np\n",
    "ar = np.array(a[0])\n",
    "ar = 255 - 1*(ar-ar.min())*(255/(ar.max()-ar.min()))\n",
    "ar = ar.astype(np.uint8)\n",
    "\n",
    "print(ar.min())\n",
    "print(ar.max())\n",
    "Image.fromarray(ar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
